---
title: "Take-home Exercise 1: Geospatial Analytics for Social Good"
editor: visual
execute: 
  warning: false
  message: false
---

## 1 Overview

Access to safe water, sanitation and hygiene is the most basic human need for health and well-being. Despite efforts in raising the access to these basic services, according to the Sustainable Development Goals Report 2022 issued by the United Nations, by 2030, 1.6 billion people will lack safely managed drinking water, 2.8 billion people will lack safely managed sanitation, and 1.9 billion people will lack basic hand hygiene facilities.

To address the issue of providing clean and sustainable water supply to the rural community, a global [Water Point Data Exchange (WPdx)](https://www.waterpointdata.org/about/) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas which then allows governments and their partners to make use of the data to improve decisions on a regular basis.

## 2 Objectives

Geospatial analytics offers a tremendous potential to solving societal problems. One such analytics is spatial autocorrelation which helps understand the degree to which one object is similar to its surrounding objects.

The objectives of this take-home exercise are as outlined in the following:

-   Using appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Any one of the three Projected Coordinate Systems of Nigeria, EPSG: 26391, 26392, and 26303 can be used.

-   Using appropriate tidyr and dplyr methods, derive the proportion of functional and non-functional water point at LGA level.

-   Combining the geospatial and aspatial data frame into simple feature data frame.

-   Performing outliers/clusters analysis by using appropriate local measures of spatial association methods.

-   Performing hotspot areas analysis by using appropriate local measures of spatial association methods.

## 3 The Data

In this exercise, we will analyse the data from Nigeria. There are 2 datasets used, as outlined in sections 3.1 and 3.2.

### 3.1 Aspatial Data

Data was downloaded from [WPdx Global Data Repositories](https://www.waterpointdata.org/access-data/) on 24 November 2022 in a csv format. The WPdx+ data set was filtered for "nigeria" in the column *clean_country_name* before downloading. There is a total of 95,08 unique water point records.

### 3.2 Geospatial Data

Nigeria Level-2 Administrative Boundary (also known as Local Government Area, LGA) polygon features GIS data was downloaded from [geoBoundaries](https://www.geoboundaries.org/).

## 4 Getting the Data Into R Environment

### 4.1 Getting Started - Setting up the environment

In the following code chunk, `p_load()` from **pacman** package is used to install and load the following R packages into the R environment:

-   sf,

-   tidyverse,

-   tmap,

-   spdep, and

-   funModeling

```{r}
pacman::p_load(sf, tmap, tidyverse, spdep, funModeling)
```

### 4.2 Import Nigeria LGA boundary data into R environment

The following code chunk uses `st_read()` from **sf** package to import the geoboundaries shapefile into R and saves the imported geospatial data into a simple feature data table.

```{r, eval = FALSE}
nga <- st_read(dsn = "geodata",
               layer = "geoBoundaries-NGA-ADM2",
               crs = 4326)
nga
```

The above printout shows the data is in wgs84 geographic coordinate system. This is the required format as we will be using `st_intersects()` later which requires the data to be in wg84 coordinate system.

In the following, `write_rds()` of **readr** package is used to save the extracted sf data table into an output file in rds format. The following code chunk saves the output file in the *geospatial* folder.

```{r, eval = FALSE}
write_rds(nga, 
          "geodata/nga.rds")
```

### 4.3 Import csv file into R environment

We will use `read_csv()` to read the csv file as shown in the following code chunk.

```{r, eval = FALSE}
wpd <- read_csv("geodata/wpdx_nigeria.csv")
```

The two fields #lat_deg and #long_deg are in decimal degree format. We will then convert `wpd` data frame into a simple feature data frame by using the following code chunk and ensuring the data has the same `wgs84` geographic coordinate system by specifying .

The two fields #lat_deg and #long_deg are in decimal degree format. As a best guess, we will assume that the data is in `wgs84` Geographic Coordinate System (i.e. the Geodetic coordinate system for World). We will then convert `wpd` data frame in to a simple feature data frame by using the following code chunk. Note that for data conversion, longitude should be supplied as the first argument in `coords` which is then followed by the argument for latitude.

```{r, eval = FALSE}
wpd_sf <- st_as_sf(wpd,
                   coords = c("#lon_deg", "#lat_deg"),
                   crs=4326) 
wpd_sf
```

From the printout above, we can see that the data is in the format that we want, i.e. `wgs84`.

Similarly, we will use `write_rds()` from **readr** package to save the extracted sf data frame into an output file in rds format. The following code chunk saves the output file in the *geopatial* folder.

```{r, eval = FALSE}
write_rds(wpd_sf, 
          "geodata/wpd_nga.rds")
```

### 4.4 Data Wrangling for Water Point Data

#### 4.4.1 Recoding NA values into string

We will then load the data in rds format. In the following code chunk, we will also rename the column from `#status_clean` to `status_clean` for easier handling in subsequent steps. In addition, `replace_na()` is used to recode all the *NA* values in `status_clean` into *unknown*.

```{r, eval = FALSE}
wp_nga <- read_rds("geodata/wpd_nga.rds") %>% 
    rename('status_clean' = '#status_clean') %>% 
    mutate(status_clean = replace_na(status_clean, "unknown"))
```

#### 4.4.2 EDA

```{r, eval = FALSE}
freq(data = wp_nga,
     input = 'status_clean')
```

It can be observed that there are different classification within functional water points and within non-functional water points. We will create 2 separate dataframes each containing either type of functional water points.

### 4.5 Extracting Water Point Data

In this section, we will extract the water point records by using classes in `status_clean` field. In the following code chunks, `filter()` from dplyr is used to select functional water points.

```{r, eval = FALSE}
wp_functional <- wp_nga %>% 
    filter(status_clean %in% 
               c("Functional",
                 "Functional but not in use",
                 "Functional but needs repair"))
```

```{r, eval = FALSE}
wp_nonfunctional <- wp_nga %>% 
    filter(status_clean %in% 
               c("Abandoned/Decommissioned",
                 "Abandoned",
                 "Non-Functional due to dry season",
                 "Non-Functional",
                 "Non functional due to dry season"))
```

```{r, eval = FALSE}
wp_unknown <- wp_nga %>% 
    filter(status_clean == "unknown")
```

To check whether the filtering was performed correctly, we can run the following code chunks and reconcile the number of records with that in Section 4.4.2.

```{r, eval = FALSE}
freq(data = wp_functional,
     input = 'status_clean')
```

```{r, eval = FALSE}
freq(data = wp_nonfunctional,
     input = 'status_clean')
```

```{r, eval = FALSE}
freq(data = wp_unknown,
     input = 'status_clean')
```

The output shows that filtering was performed successfully.

### 4.6 Performing Point-in-Polygon Count

Next, we want to find the number of functional water points in each LGA as well as the number of total, functional, non-functional, and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using `st_intersects()`. Next, `length()` is used to calculate the number of functional water points that fall inside each LGA.

```{r, eval = FALSE}
nga_wp <- nga %>% 
  mutate(`total_wp` = lengths(
    st_intersects(nga, wp_nga))) %>%
  mutate(`wp_functional` = lengths(
    st_intersects(nga, wp_functional))) %>%
  mutate(`wp_nonfunctional` = lengths(
    st_intersects(nga, wp_nonfunctional))) %>%
  mutate(`wp_unknown` = lengths(
    st_intersects(nga, wp_unknown)))
```

### 4.7 Saving the Analytical Data Table

We will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, `mutate()` from dplyr package is used to derive two fields, namely *pct_functional* and *pct_nonfunctional*

```{r, eval = FALSE}
nga_wp <- nga_wp %>% 
    mutate(pct_functional = wp_functional/total_wp) %>% 
    mutate(pct_non_functional = wp_nonfunctional/total_wp)
```

Now that we have the tidy sf data table, we will save it in rds format for subsequent analysis.

```{r, eval = FALSE}
write_rds(nga_wp, "geodata/nga_wp.rds")
```

## 5 Visualising the Spatial Distribution of Waterpoints

We will visualise the spatial distribution of function and non-functional water points using a choropleth. This is performed using the code chunk below.

```{r fig.height = 5, fig.width = 5}
nga_wp <- read_rds("geodata/nga_wp.rds")
total <- qtm(nga_wp, "total_wp")
wp_functional <- qtm(nga_wp, "wp_functional")
wp_nonfunctional <- qtm(nga_wp, "wp_nonfunctional")
unknown <- qtm(nga_wp, "wp_unknown")

tmap_arrange(total, wp_functional, wp_nonfunctional, unknown, asp=1, ncol=2)
```

## 6 Global Spatial Autocorrelation

In this section, we will compute global spatial autocorrelation statistics and perform spatial complete randomness test for global spatial autocorrelation. Global spatial autocorrelation describes the presence of systematic spatial variation in a variable (in this case, proportion of functional water points and proportion of non-functional water points) in the study area (i.e. Nigeria) as a whole. We will evaluate two global spatial autocorrelation statistics - Moran's I and Geary's C.

### 6.1 Computing Contiguity Spatial Weights

We will first identify the spatial weights which is used to define the neighbourhood relationship between the geographical units.

There are 2 main approaches to compute the spatial weights, namely, the contiguity approach and the distance approach. In the contiguity approach, neighbours are identified to be geographical areas that share a common boundary. In the Rook's criteria, areas need to have perfect shared boundary in order to be considered as neighbours, whereas for Queen's criteria, areas that have either perfect shared boundary or diagonal shared boundary are considered as neighbours. However, in the case for Nigeria, we can observe that the LGAs are not approximately uniform. Using the contiguity approach may result in some LGAs to have more neighbours and some LGAs to have less neighbours - resulting in underestimating and overestimating the contributions of their neighbours respectively. As such, the contiguity approach is not suitable.

In the distance method, there are 2 approaches - (1) fixed distance approach where areas are identified to be neighbours if the distance between their centroids are within the fixed distance and (2) adaptive weighting scheme where shorter bandwidths (or distances) are used when data is dense and longer bandwidths for data that is sparse. One advantage of the adaptive distance weight scheme is that we can control the number of neighbours by using k-nearest neighbours. To use fixed distance, the regions should be of similar size so that the centroid represent each region well. Since the LGAs in Nigeria do not have similar sizes, fixed distance approach is not suitable. Another area where fixed distance works well is when there are very large polygons at the edge of the study area and very small polygons at the center, which again, is not observed for Nigeria. As such, we will use th adaptive weighting scheme.

#### 6.1.1 Retrieving longitude and latutide of polygon centroids

We will first need to associate each polygon with a point in order to determine the nearest neighbours. The most typical method for this is the polygon centroids which gives us the longitude and the latitude of each LGA.

In the following code chunk, we use `map_dbl()` to transform the geometry of each LGA (represented by *nga_wp\$geometry*) by applying the function `st_centroid()` to each LGA. We then access the longitude using \[\[1\]\].

```{r}
longitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]])
```

Likewise, we perform the following to access the latitude of the LGAs, this time using \[\[2\]\] to access the latitude.

```{r}
latitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]])
```

Now that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.

```{r}
coords <- cbind(longitude, latitude)
```

We check the first few observations to see if things are formatted correctly.

```{r}
head(coords)
```

#### 6.1.2 Computing adaptive distance weight matrix

In the following code chunk, we define k = 8 to find the k-nearest neighbours using `knearineigh()` and `knn2nb()` to return a list of integer vectors containing neighbour region number ids.

```{r}

knn8 <- knn2nb(knearneigh(coords, k = 8))
knn8
```

The following code chunk allows us to display the content of the mstrix using `str()`.

```{r}
str(knn8)
```

We can visualise the weight matrix using the code chunk below.

```{r fig.height = 5, fig.width = 5}
plot(nga_wp$geometry, border = "lightgrey")
plot(knn8, coords, pch = 10, cex = 0.5, add = TRUE, col = "red")
```

#### 6.1.3 Binary weights matrix

Next, we will assign weights to each neighboring polygon by using the basic binary coding.

In the following code chunk, the input of *nb2listw()* must be an object of class **nb**. The syntax of the function has two major arguments, namely style and zero.poly.

-   We defined style = "B" which is binary coding assignment where neighbours are given a value of 1 and non-neighbours are given a value of 0.

-   If *zero policy* is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list.

```{r}
bwm <- nb2listw(knn8,
                style = "B",
                zero.policy = TRUE)
bwm
```

### 6.2 Global Spatial Autocorrelation: Moran's I

In this section, we will demonstrate how to perform Moran's I statistics testing by using [*moran.test()*](https://r-spatial.github.io/spdep/reference/moran.test.html) of **spdep**. Moran's I is a test for spatial autocorrelation. It measures the overall spatial autocorrelation of the data, i.e. overall, how one object is similar or dissimilar to others surrounding it, evaluating whether the observation (in our case, *values for the proportion of non-functional water points*) is clustered, dispersed, or random.

The values of Moran's I range from +1 meaning strong positive spatial autocorrelation (clustering) to 0 meaning a random pattern to -1 indicating strong negative spatial autocorrelation (dispersion).

#### 6.2.1 Moran's I test

The null hypothesis we are testing states that "*The values for the proportion of non-functional water points are randomly distributed across counties, following a completely random process".* The alternative hypothesis is"*The values for the proportion of non-functional water points is not randomly dispersed, i.e. it is clustered in noticeable patterns*".

The following code chunk performs Moran's I statistic test using [*moran.test()*](https://r-spatial.github.io/spdep/reference/moran.test.html) of **spdep**.

```{r}
moran.test(nga_wp$pct_non_functional, 
           listw = bwm, 
           zero.policy = TRUE, 
           na.action = na.omit)
```

Since p-value is very small, \< 0.05 (statistically significant) and the Moran I statistic (0.43977) is positive, we reject the null hypothesis and conclude that the *values for the proportion of non-functional water points* is spatially clustered.

#### 6.2.2 Computing Monte Carlo Moran's I

The Moran's I analysis benefits from being fast. But it may be sensitive to irregularly distributed polygons. A safer approach to hypothesis testing is to run a Monte Carlo simulation using the `moran.mc()` function. The `moran.mc` function takes an extra argument *n*, the number of simulations.

The code chunk below performs permutation test for Moran's I statistic by using [*moran.mc()*](https://r-spatial.github.io/spdep/reference/moran.mc.html) of **spdep**. A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm = moran.mc(nga_wp$pct_non_functional, 
                listw = bwm, 
                nsim = 999, 
                zero.policy = TRUE, 
                na.action = na.omit)
bperm
```

The Monte Carlo simulation generates a very small p-value, i.e. \< 0.05 (thus statistically significant). Again, we can reject the null hypothesis and conclude that overall, the *values for the proportion of non-functional water points* is spatially clustered.

#### 6.2.3 Visualising Monte Carlo Moran's I

To examine the simulated Moran's I test statistics in greater detail, we can plot the distribution of the statistical values as a histogram by using the following code chunk.

```{r}
hist(bperm$res, 
     freq = TRUE, 
     breaks = 50, 
     xlab = "Simulated Moran's I",
     main = "Histogram of Monte Carlo Simulation of Moran's I")
abline(v=0, 
       col="red") 
abline(v=0.44, 
       col="blue") 
text(0.37, 180, "Moran's I value = 0.44", cex = 0.8, col='blue')
```

The Moran's I value (represented by the blue vertical line) is far outside the simulated data (grey shaded region) which indicates a statistically significant relationship. \[1\]

### 6.3 Global Spatial Autocorrealtion: Geary's C

Geary's C is a measure of spatial autocorrelation or an attempt to determine if adjacent observations of the same phenomenon are correlated. How this differs from Moran's I is that in general, Moran's I is a measure of global spatial autocorrelation, while Geary's C is more sensitive to local spatial autocorrelation. Geary's C is also known as Geary's contiguity ratio or simply Geary's ratio.

A Geary's C statistic close to 1 indicates that there is no significant autocorrelation between observation i and its neighbors, where Geary's C statistic \< 1 indicates that the observation has neighbors which are significantly similar to it (positive spatial autocorrelation). Likewise, Geary's C statistic \> 1, demonstrates that the observation is among neighbors which differ significantly from it (negative spatial autocorrelation). \[2\]

#### 6.3.1 Geary's C test

In Geary's C test, we define the null hypothesis "There is no association between the *values for the proportion of non-functional water points* observed at a location and values observed at nearby LGAs". The alternative hypothesis is "Nearby sites have either similar or dissimilar *values for the proportion of non-functional water points*". The code chunk below perform Geary's C test for spatial autocorrelation by using geary.test() from spdep.

```{r eval = FALSE}
geary.test(nga_wp$pct_non_functional, 
           listw = bwm, 
           zero.policy = TRUE)
```

#### 6.3.2 Computing Monte Carlo Geary's C

#### 6.3.3 Visualising the Monte Carlo Geary's C

### 6.4 Spatial Correlogram

Spatial correlograms are great to examine patterns of spatial autocorrelation in the data or model residuals. They show how correlated are pairs of spatial observations as the distance (lag) between them increases - they are plots of some index of autocorrelation (Moran's I) against distance. Spatial correlograms serve as very useful exploratory and descriptive tool.

#### 6.4.1 Compute Moran's I correlogram

In the following code chunk, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran's I. The plot() of base Graph is then used to plot the output.

```{r, eval = FALSE}
MI_corr <- sp.correlogram(knn8, 
                          nga_wp$pct_non_functional, 
                          order = 6, 
                          method = "I", 
                          style = "B")
plot(MI_corr)
```

## 7 Local Spatial Autocorrelation

The Cluster and Outlier Analysis (Anselin Local Moran's I) tool identifies concentrations of high values, concentrations of low values, and spatial outliers. \[3\] It can help to answer questions such as:

-   Where are the sharpest boundaries between LGA with high proportion of non-functional water points and LGA with low proportion of non-functional water points?

-   Are there locations where there is a high number of LGAs with high proportion of non-functional water points?

A positive value for I indicates that a feature has neighboring features with similarly high or low attribute values; this feature is part of a cluster. A negative value for I indicates that a feature has neighboring features with dissimilar values; this feature is an outlier. In either instance, the p-value for the feature must be small enough for the cluster or outlier to be considered statistically significant.

### 7.1 Computing local Moran's I

To compute local Moran\'s I, the [*localmoran()*](https://r-spatial.github.io/spdep/reference/localmoran.html) function of **spdep** will be used. It computes *Ii* values, given a set of *zi* values and a listw object providing neighbour weighting information for the polygon associated with the zi values.

The code chunks below are used to compute local Moran\'s I of *values for proportion of non-functional water points* at the LGA level.

```{r eval = FALSE}
fips <- order(nga_wp$shapeName)
localMI <- localmoran(nga_wp$pct_non_functional, bwm)
head(localMI)
```

The code chunk below list the content of the local Moran matrix derived by using [*printCoefmat()*](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/printCoefmat).

```{r eval = FALSE}
printCoefmat(data.frame(localMI[fips,], row.names=hunan$County[fips]), check.names=FALSE)
```

Reference:

\[3\] [Understanding Anselin Local Moran's I](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-cluster-and-outlier-analysis-anselin-local-m.htm#:~:text=Potential%20applications-,The%20Cluster%20and%20Outlier%20Analysis%20(Anselin%20Local%20Moran's%20I)%20tool,poverty%20in%20a%20study%20area%3F)
